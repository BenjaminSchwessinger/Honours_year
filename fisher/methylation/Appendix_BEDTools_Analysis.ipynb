{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out BEDTools analysis steps\n",
    "\n",
    "This notebook contains code that was used to plan out and troubleshoot issues in the main BEDTools analysis folder. It has the following parts:\n",
    "\n",
    "- 3A. Cutoffs from intersect files\n",
    "- 4. Windows\n",
    "- 5B. Running coverage on test dataset (pcontig_019)\n",
    "- 5C. 5D. Testing out loop to save coverage files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybedtools\n",
    "from pybedtools import BedTool\n",
    "import os\n",
    "import glob\n",
    "import pprint\n",
    "import numpy # need for p-value stats\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we need to define the base dirs\n",
    "DIRS ={}\n",
    "DIRS['BASE1'] = '/home/anjuni/methylation_calling/pacbio'\n",
    "DIRS['BASE2'] = '/home/anjuni/analysis'\n",
    "DIRS['BED_INPUT'] = os.path.join(DIRS['BASE2'], 'bedtools_output', 'sequencing_comparison')\n",
    "DIRS['GFF_INPUT'] = os.path.join(DIRS['BASE2'], 'gff_output')\n",
    "DIRS['WINDOW_OUTPUT'] = os.path.join(DIRS['BASE2'], 'windows')\n",
    "DIRS['WINDOW_INPUT'] = os.path.join(DIRS['BASE2'], 'input_for_windows')\n",
    "DIRS['REF'] = '/home/anjuni/Pst_104_v13_assembly/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Venn diagram\n",
    "# First way to call the 2 group Venn diagram:\n",
    "#PacBio sites:\n",
    "#88932\n",
    "#Overlapping sites:\n",
    "#84733\n",
    "#Nanopore sites:\n",
    "#83451878\n",
    "\n",
    "c = 84733\n",
    "a = 83451878-c\n",
    "b = 88932-c\n",
    "venn2(subsets = (a, b, c), set_labels = ('Nanopore', 'Pacbio'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:#8a14ff'> 3. Making cutoff files. <span/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#8a14ff'> 3.A Making cutoff files for overlapping files from previous section. <span/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "#Move the tombo hc files to the 'sequencing_comparison' folder with the other overlapped files to continue analysis\n",
    "cd /home/anjuni/methylation_calling/pacbio/input/sorted_bed_files/\n",
    "cp 5mC_hc_tombo_sorted.bed ~/analysis/bedtools_output/sequencing_comparison/\n",
    "cp 6mA_hc_tombo_sorted.bed ~/analysis/bedtools_output/sequencing_comparison/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "#Move the alternative bed intersect files and older tombo-np intersect files to a separate folder\n",
    "#There will be 4 files remaining that will be used for the rest of the analysis\n",
    "cd /home/anjuni/analysis/bedtools_output/sequencing_comparison/\n",
    "mkdir alt_bed\n",
    "mv 6mA_pb_ont.bed alt_bed\n",
    "mv *np_tombo* alt_bed\n",
    "mv 5mC_tombo_np.bed alt_bed/\n",
    "mv 5mC_CpG_tombo_np.bed alt_bed/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make filepaths for both 6mA files, both CpG files, and the tombo file\n",
    "bed_file_list = ['/home/anjuni/analysis/bedtools_output/sequencing_comparison/6mA_ont_pb.bed', \\\n",
    "                 '/home/anjuni/analysis/bedtools_output/sequencing_comparison/6mA_pb_ont.bed', \\\n",
    "                 '/home/anjuni/analysis/bedtools_output/sequencing_comparison/5mC_CpG_tombo_np.bed', \\\n",
    "                 '/home/anjuni/analysis/bedtools_output/sequencing_comparison/5mC_CpG_np_tombo.bed', \\\n",
    "                 '/home/anjuni/analysis/bedtools_output/sequencing_comparison/5mC_hc_tombo_sorted.bed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the list of cutoffs\n",
    "cutoff_list = [1.00, 0.99, 0.95, 0.90, 0.80, 0.70, 0.60, 0.50, 0.40, 0.30, 0.20, 0.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to filter\n",
    "def score_filter(feature, L):\n",
    "    \"\"\"Returns True if feature is longer than L\"\"\"\n",
    "    return float(feature.score) >= L\n",
    "\n",
    "def filter_by_cutoffs(bed_files, cutoffs, initial_file_path, final_file_path):\n",
    "    \"\"\"Filters files by the list of cutoffs given, and renames the file according to the cutoff.\"\"\"\n",
    "    for file in bed_files:\n",
    "        pybed_object = BedTool(file)\n",
    "        for x in cutoffs:\n",
    "            filtered_file = pybed_object.filter(score_filter, x)\n",
    "            cutoff = \"{:.2f}\".format(x)\n",
    "            cutoff_name = '.cutoff.' + cutoff + '.bed'\n",
    "            out_filename = file.replace('.bed', cutoff_name)\n",
    "            out_filename = out_filename.replace(initial_file_path, final_file_path)\n",
    "            filtered_file.saveas(out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the function to filter all files\n",
    "initial_fp = '/home/anjuni/analysis/bedtools_output/sequencing_comparison/'\n",
    "final_fp = '/home/anjuni/analysis/bedtools_output/cutoffs_from_intersects/'\n",
    "filter_by_cutoffs(bed_file_list, cutoff_list, initial_fp, final_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "### Optional\n",
    "# Move the original files to this folder as well, and change their names, for the 0.00 cutoff, if it was not done earlier\n",
    "cd /home/anjuni/methylation_calling/pacbio/input/sorted_bed_files/cutoffs_5mC\n",
    "for x in /home/anjuni/methylation_calling/pacbio/input/sorted_bed_files/5*\n",
    "do\n",
    "cp ${x} .\n",
    "done\n",
    "\n",
    "mv 5mC_hc_nanopolish_sorted.bed 5mC_hc_nanopolish_sorted.cutoff.0.00.bed\n",
    "mv 5mC_hc_tombo_sorted.CpG.plus.bed 5mC_hc_tombo_sorted.CpG.plus.cutoff.0.00.bed\n",
    "mv 5mC_hc_tombo_sorted.bed 5mC_hc_tombo_sorted.cutoff.0.00.bed\n",
    "\n",
    "cd /home/anjuni/methylation_calling/pacbio/input/sorted_bed_files/cutoffs_6mA\n",
    "for y in /home/anjuni/methylation_calling/pacbio/input/sorted_bed_files/6*\n",
    "do\n",
    "cp ${y} .\n",
    "done\n",
    "\n",
    "mv 6mA_prob_smrtlink_sorted.bed 6mA_prob_smrtlink_sorted.cutoff.0.00.bed\n",
    "mv 6mA_hc_tombo_sorted.bed 6mA_hc_tombo_sorted.cutoff.0.00.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Optional\n",
    "# Run intersect cutoffs for the original 0-cutoff files post-hoc\n",
    "ont_final = ['/home/anjuni/methylation_calling/pacbio/input/sorted_bed_files/cutoffs_6mA/6mA_hc_tombo_sorted.cutoff.0.00.bed']\n",
    "pb_final = ['/home/anjuni/methylation_calling/pacbio/input/sorted_bed_files/cutoffs_6mA/6mA_prob_smrtlink_sorted.cutoff.0.00.bed']\n",
    "intersect_cutoffs(ont_final, pb_final, 1, '6mA', 'tombo', 'smrtlink')\n",
    "\n",
    "tmb_final = ['/home/anjuni/methylation_calling/pacbio/input/sorted_bed_files/cutoffs_5mC/5mC_hc_tombo_sorted.CpG.plus.cutoff.0.00.bed']\n",
    "np_final = ['/home/anjuni/methylation_calling/pacbio/input/sorted_bed_files/cutoffs_5mC/5mC_hc_nanopolish_sorted.cutoff.0.00.bed']\n",
    "intersect_cutoffs(tmb_final, np_final, 1, '5mC', 'tombo', 'nanopolish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#8a14ff'> 3.B Making random distributions of the same size as all the cutoff files. <span/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_sites(all_site_fn, number_of_subset_sites, out_fn):\n",
    "    \"\"\"This is a function that creates a BED file containing a subset of the total sites in the genome, with the size of the subset equal to an input cutoff file.\"\"\"\n",
    "    df = pd.read_csv(all_site_fn, sep='\\t', header = None) #use up memory making a dataframe of the csv file, to avoid running through it for each base in each loop\n",
    "    random_array = np.random.choice(df.shape[0], number_of_subset_sites)\n",
    "    random_array.sort()\n",
    "    df.iloc[random_array, :].to_csv(out_fn, header=None, index=None, sep='\\t') #iloc[rows, columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing out the subset_function\n",
    "\n",
    "# on an input file\n",
    "all_site_fn = all_a\n",
    "number_of_all_sites = count_dict['6mA_tombo_sorted.bed']\n",
    "number_of_subset_sites = count_dict['6mA_prob_smrtlink_sorted.cutoff.0.99.bed']\n",
    "out_fn = '6mA_prob_smrtlink_sorted.cutoff.0.99.bed'.replace('.bed', '.rand_1.bed')\n",
    "subset_sites(all_site_fn, number_of_subset_sites, out_fn)\n",
    "\n",
    "# on 10 sites only\n",
    "subset_sites(all_site_fn, 10, 'test_10.bed')\n",
    "\n",
    "#on a list of subset sizes\n",
    "subset_sites(all_site_fn, [50, 11], ['test_10.bed','test_11.bed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative to making a dictionary of site count\n",
    "def count_sites_list(file_list):\n",
    "    \"\"\"Returns a list of the number of sites in each cutoff file, in order.\"\"\"\n",
    "    count_list = []\n",
    "    for file in file_list:\n",
    "        with open(file) as in_file:\n",
    "            count = 0\n",
    "            for line in in_file:\n",
    "                count += 1\n",
    "        count_list.append(count)\n",
    "    return count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset with list instead of dict\n",
    "def subset_sites(all_site_fn, number_of_subset_sites, out_fn):\n",
    "    \"\"\"This is a function that creates a BED file containing a subset of the total sites in the genome, with the size of the subset equal to an input cutoff file.\"\"\"\n",
    "    df = pd.read_csv(all_site_fn, sep='\\t', header = None) #use up memory making a dataframe of the csv file, to avoid running through it for each base in each loop\n",
    "    if type(number_of_subset_sites) == int and type(out_fn) == str:\n",
    "        random_array = np.random.choice(df.shape[0], number_of_subset_sites)\n",
    "        random_array.sort()\n",
    "        df.iloc[random_array, :].to_csv(out_fn, header=None, index=None, sep='\\t') #iloc[rows, columns]\n",
    "    elif type(number_of_subset_sites) == list and type(out_fn) == list:\n",
    "        for n, out in zip(number_of_subset_sites, out_fn): # have sorted lists to zip properly\n",
    "            random_array = np.random.choice(df.shape[0], n)\n",
    "            random_array.sort()\n",
    "            df.iloc[random_array, :].to_csv(out_fn, header=None, index=None, sep='\\t') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dict function\n",
    "\n",
    "test_n = {}\n",
    "test_n['any'] = intersect_count_dict['6mA_tombo_smrtlink.cutoff.0.99.bed']\n",
    "test_n['thing'] = intersect_count_dict['6mA_tombo_smrtlink.cutoff.0.95.bed']\n",
    "\n",
    "test_f = {}\n",
    "test_f['any'] = '/home/anjuni/analysis/coverage/test_randomisation/6mA_tombo_smrtlink.cutoff.0.99_rand.bed'\n",
    "test_f['thing'] = '/home/anjuni/analysis/coverage/test_randomisation/6mA_tombo_smrtlink.cutoff.0.95_rand.bed'\n",
    "\n",
    "all_a = os.path.join(DIRS['BASE1'], 'input', 'sorted_bed_files', '6mA_tombo_sorted.bed')\n",
    "\n",
    "new_subset_sites(all_a, test_n, test_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dictionaries by group instead of modification\n",
    "# Make dictionaries of the file name and file size\n",
    "intersect_count_dict = count_sites(intersect_files)\n",
    "tombo_count_dict = count_sites(tombo_files)\n",
    "\n",
    "pprint.pprint(intersect_count_dict)\n",
    "pprint.pprint(tombo_count_dict)\n",
    "\n",
    "# Make dictionaries of all out files\n",
    "intersect_rand_dict = out_rand_files(intersect_files)\n",
    "tombo_rand_dict = out_rand_files(tombo_files)\n",
    "\n",
    "pprint.pprint(intersect_rand_dict)\n",
    "pprint.pprint(tombo_rand_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternate method\n",
    "\n",
    "# Make a list of cutoff files and tombo files\n",
    "intersect_files = intersected_5mC + intersected_6mA\n",
    "tombo_files = hc_tombo_5mC + hc_tombo_6mA\n",
    "\n",
    "# Make a dictionary of all the input file names and file handles\n",
    "cutoff_fn_dict = {}\n",
    "for i in intersect_files:\n",
    "    cutoff_fn_dict[i.split('/')[-1]] = i\n",
    "    \n",
    "tombo_fn_dict = {}\n",
    "for i in tombo_files:\n",
    "    tombo_fn_dict[i.split('/')[-1]] = i\n",
    "    \n",
    "# Make dictionary for total C, total A, and total (+) strand CpG in genome\n",
    "all_count_dict = count_sites([all_c, all_a, all_cpg])\n",
    "print(all_count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = intersect_6mA_dict['6mA_tombo_smrtlink.cutoff.0.99.bed']\n",
    "exp = intersect_6mA_rand_dict['6mA_tombo_smrtlink.cutoff.0.99.bed']\n",
    "\n",
    "print(obs)\n",
    "print(exp)\n",
    "\n",
    "df1 = pd.read_csv(obs, sep='\\t', header = None)\n",
    "df2 = pd.read_csv(obs, sep='\\t', header = None)\n",
    "\n",
    "df1.head()\n",
    "\n",
    "obss = df1[4]\n",
    "expp = df2[4]\n",
    "\n",
    "len(expp)\n",
    "\n",
    "stat, p = chisquare(obss, expp)\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "dict1 = {}\n",
    "dict1['name'] = stat, p\n",
    "\n",
    "dict1['name']\n",
    "\n",
    "stat = chisquare(obss, expp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:#144fff'> 4. Making windows. <span/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all file paths for window BED files\n",
    "window_fn_dict = {}\n",
    "window_bed_dict = {}\n",
    "window_fn_dict['100kb'] = os.path.join(DIRS['WINDOW_OUTPUT'], 'Pst_104E_v13_ph_ctg_w100kb.bed')\n",
    "window_fn_dict['30kb'] = os.path.join(DIRS['WINDOW_OUTPUT'], 'Pst_104E_v13_ph_ctg_w30kb.bed')\n",
    "window_fn_dict['10kb'] = os.path.join(DIRS['WINDOW_OUTPUT'], 'Pst_104E_v13_ph_ctg_w10kb.bed')\n",
    "window_fn_dict['100kb_s20kb'] = os.path.join(DIRS['WINDOW_OUTPUT'], 'Pst_104E_v13_ph_ctg_w100kb_s20kb.bed')\n",
    "window_fn_dict['30kb_s6kb'] = os.path.join(DIRS['WINDOW_OUTPUT'], 'Pst_104E_v13_ph_ctg_w30kb_s6kb.bed')\n",
    "window_fn_dict['10kb_s2kb'] = os.path.join(DIRS['WINDOW_OUTPUT'], 'Pst_104E_v13_ph_ctg_w10kb_s2kb.bed')\n",
    "genome_size_f_fn = os.path.join(DIRS['WINDOW_INPUT'], 'Pst_104E_v13_ph_ctg.sorted.genome_file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:#148aff'> 5. Coverage analysis of methylation with gene annotation files. <span/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#148aff'> 5.B Run converage analysis on test dataset (pcontig_019). <span/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make filepaths for feature files for genes, effectors, TE, methylation\n",
    "feature_fn_dict = {}\n",
    "feature_fn_dict['genes'] = gene_fn\n",
    "feature_fn_dict['TE'] = te_fn\n",
    "feature_fn_dict['effector'] = os.path.join(DIRS['WINDOW_INPUT'], 'Pst_104E_v13_ph_ctg.effectors.gff3' )\n",
    "feature_fn_dict['ont_6mA_0.10'] = ont_6mA[0]\n",
    "feature_fn_dict['pb_6mA_0.10'] = pb_6mA[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the function file dictionary works (it does)\n",
    "pprint.pprint(feature_fn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary of feature files\n",
    "feature_bed_dict = {}\n",
    "for key, value in feature_fn_dict.items():\n",
    "    feature_bed_dict[key] = BedTool(value)\n",
    "    \n",
    "# Check whether the function bed dictionary works (it does)\n",
    "pprint.pprint(feature_bed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Make a subset of windows from pcontig_019 as a test dataset\n",
    "cd /home/anjuni/analysis/windows/\n",
    "for x in *.bed\n",
    "do\n",
    "len=${#x}\n",
    "name=${x::len-4}\n",
    "echo ${name}\n",
    "grep 'pcontig_019' ${x} > test_windows/${name}.pcontig_019.bed\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a filepath dictionary and a bed file dictionary of the test windows\n",
    "test_window_fn_dict = {}\n",
    "test_window_fn_dict['100kb'] = os.path.join(DIRS['WINDOW_OUTPUT'], 'test_windows', 'Pst_104E_v13_ph_ctg_w100kb.pcontig_019.bed')\n",
    "test_window_fn_dict['10kb'] = os.path.join(DIRS['WINDOW_OUTPUT'], 'test_windows', 'Pst_104E_v13_ph_ctg_w10kb.pcontig_019.bed')\n",
    "test_window_fn_dict['30kb'] = os.path.join(DIRS['WINDOW_OUTPUT'], 'test_windows', 'Pst_104E_v13_ph_ctg_w30kb.pcontig_019.bed')\n",
    "\n",
    "test_window_bed_dict = {}\n",
    "for key, value in test_window_fn_dict.items():\n",
    "    test_window_bed_dict[key] = BedTool(value)\n",
    "\n",
    "pprint.pprint(test_window_bed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary of sliding windows\n",
    "test_sliding_window_fn_dict = {}\n",
    "test_sliding_window_fn_dict['100kb_s20kb'] = os.path.join(DIRS['WINDOW_OUTPUT'], 'test_windows', 'Pst_104E_v13_ph_ctg_w100kb_s20kb.pcontig_019.bed')\n",
    "test_sliding_window_fn_dict['10kb_s2kb'] = os.path.join(DIRS['WINDOW_OUTPUT'], 'test_windows', 'Pst_104E_v13_ph_ctg_w10kb_s2kb.pcontig_019.bed')\n",
    "test_sliding_window_fn_dict['30kb_s6kb'] = os.path.join(DIRS['WINDOW_OUTPUT'], 'test_windows', 'Pst_104E_v13_ph_ctg_w30kb_s6kb.pcontig_019.bed')\n",
    "\n",
    "test_sliding_window_bed_dict = {}\n",
    "for key, value in test_sliding_window_fn_dict.items():\n",
    "    test_sliding_window_bed_dict[key] = BedTool(value)\n",
    "    \n",
    "pprint.pprint(test_sliding_window_bed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Test out overlaps for test dataset on command line, to see what output looks like (works)\n",
    "cd /home/anjuni/analysis/windows/test_windows\n",
    "features=/home/anjuni/analysis/gff_output\n",
    "methyl=/home/anjuni/methylation_calling/pacbio/input/sorted_bed_files/cutoffs_6mA\n",
    "ont_6mA_100kb=100kb_6mA_hc_tombo_0.10.bed\n",
    "pb_6mA_100kb=100kb_6mA_prob_smrtlink_0.10.bed\n",
    "\n",
    "coverageBed -a Pst_104E_v13_ph_ctg_w100kb.pcontig_019.bed -b ${methyl}/6mA_hc_tombo_sorted.cutoff.0.10.bed > 100kb_6mA_hc_tombo_0.10.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Test out the histogram function in coverageBed, to see what output looks like. No need to use it\n",
    "cd /home/anjuni/analysis/windows/test_windows\n",
    "features=/home/anjuni/analysis/gff_output\n",
    "methyl=/home/anjuni/methylation_calling/pacbio/input/sorted_bed_files/cutoffs_6mA\n",
    "coverageBed -a Pst_104E_v13_ph_ctg_w100kb.pcontig_019.bed -b ${methyl}/6mA_hc_tombo_sorted.cutoff.0.10.bed -hist > h100kb_6mA_hc_tombo_0.10.bed\n",
    "\n",
    "# It just puts a row for all(?) at the bottom?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#148aff'> 5.C Testing out Ben's pybedtools coverage function. <span/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out Ben's function to see if it's easier?\n",
    "# make a dataframe to put headings\n",
    "# (the function kwarg .coverage(F=0.1) indicates minimum fraction overlap)\n",
    "tmp_df = test_window_bed_dict['100kb'].coverage(test_feature_fn_dict['ont_6mA_0.10']).to_dataframe().iloc[:,[0,1,2,3,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataframe\n",
    "tmp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename headings\n",
    "tmp_df.rename(columns={'name': 'overlap_count', 'thickStart': 'overlap_fraction'}, inplace=True)\n",
    "tmp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change output file path\n",
    "tmp_fn = test_feature_fn_dict['ont_6mA_0.10'].replace('.bed', '.%s.overlap.bed' % '100kb')\n",
    "tmp_fn = tmp_fn.replace('test_feature_files', 'test_coverage')\n",
    "print(tmp_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary for overlap file name as key and dataframe as value\n",
    "feature_overlap_df_dict = {}\n",
    "feature_overlap_df_dict[tmp_fn.split('/')[-1]] = tmp_df\n",
    "pprint.pprint(feature_overlap_df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to a csv (note: pybedtools has more decimal places than bash bedtools)\n",
    "tmp_df.to_csv(tmp_fn, sep='\\t', header=None, index=None) # no headers or row names in csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:#148aff'> 5.D Using Ben's pybedtools coverage function on test window dataset. <span/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dictionary of test feature files to make the function easier\n",
    "# The necessary feature files were moved to their own folder first\n",
    "# Will need to move all feature files to a folder for actual analysis\n",
    "\n",
    "DIRS['TEST_COV'] = os.path.join(DIRS['BASE2'], 'coverage/test_feature_files')\n",
    "test_feature_fn_dict = {}\n",
    "test_feature_fn_dict['genes'] = os.path.join(DIRS['TEST_COV'], 'Pst_104E_v13_ph_ctg.anno.sorted.gff3')\n",
    "test_feature_fn_dict['TE'] = os.path.join(DIRS['TEST_COV'], 'Pst_104E_v13_ph_ctg.TE.sorted.gff3')\n",
    "test_feature_fn_dict['effector'] = os.path.join(DIRS['TEST_COV'], 'Pst_104E_v13_ph_ctg.effectors.gff3')\n",
    "test_feature_fn_dict['ont_6mA_0.10'] = os.path.join(DIRS['TEST_COV'], '6mA_hc_tombo_sorted.cutoff.0.10.bed')\n",
    "test_feature_fn_dict['pb_6mA_0.10'] = os.path.join(DIRS['TEST_COV'], '6mA_prob_smrtlink_sorted.cutoff.0.10.bed')\n",
    "test_feature_fn_dict['tmb_5mC_0.10'] = os.path.join(DIRS['TEST_COV'], '5mC_hc_tombo_sorted.cutoff.0.10.bed')\n",
    "test_feature_fn_dict['tmb_cpg_5mC_0.10'] = os.path.join(DIRS['TEST_COV'], '5mC_hc_tombo_sorted.CpG.cutoff.0.10.bed')\n",
    "test_feature_fn_dict['np_5mC_0.10'] = os.path.join(DIRS['TEST_COV'], '5mC_hc_nanopolish_sorted.cutoff.0.10.bed')\n",
    "\n",
    "# test dict\n",
    "pprint.pprint(test_feature_fn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary of bed test feature files and view\n",
    "test_feature_bed_dict = {}\n",
    "for key, value in test_feature_fn_dict.items():\n",
    "    test_feature_bed_dict[key] = BedTool(value)\n",
    "\n",
    "pprint.pprint(test_feature_bed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so the gff3 files don't get renamed properly, so I may need to make a new loop for them, or add an \"if\" line for them\n",
    "test_feature_overlap_df_dict = {}\n",
    "for wkey, wbed in test_window_bed_dict.items():\n",
    "    for fkey, fbed in test_feature_bed_dict.items():\n",
    "        tmp_df = wbed.coverage(fbed).to_dataframe().iloc[:,[0,1,2,3,6]] # make a dataframe to put headings\n",
    "        tmp_df.rename(columns={'name': 'overlap_count', 'thickStart': 'overlap_fraction'}, inplace=True) # rename headings\n",
    "        if tmp_fn.endswith('.bed'):\n",
    "            tmp_fn = test_feature_fn_dict[fkey].replace('.bed', '.%s.overlap.bed' % wkey) # change output file path\n",
    "        if tmp_fn.endswith('.gff3'):\n",
    "            tmp_fn = test_feature_fn_dict[fkey].replace('.gff3', '.%s.overlap.bed' % wkey) # change output file path\n",
    "        tmp_fn = tmp_fn.replace('test_feature_files', 'test_coverage')\n",
    "        test_feature_overlap_df_dict[tmp_fn.split('/')[-1]] = tmp_df # file name as key and dataframe as value for overlap dict\n",
    "        tmp_df.to_csv(tmp_fn, sep='\\t', header=None, index=None) # save to a csv(pybedtools outputs more d.p. than BEDTools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The methylation files were too low-quality, so coverage for higher quality methylation data was collected :)\n",
    "hc_feature_fn_dict = {}\n",
    "hc_feature_fn_dict['ont_6mA_0.90'] = os.path.join(DIRS['TEST_COV'], '6mA_hc_tombo_sorted.cutoff.0.90.bed')\n",
    "hc_feature_fn_dict['ont_6mA_0.50'] = os.path.join(DIRS['TEST_COV'], '6mA_hc_tombo_sorted.cutoff.0.50.bed') \n",
    "hc_feature_fn_dict['pb_6mA_0.90'] = os.path.join(DIRS['TEST_COV'], '6mA_prob_smrtlink_sorted.cutoff.0.90.bed')\n",
    "hc_feature_fn_dict['pb_6mA_0.50'] = os.path.join(DIRS['TEST_COV'], '6mA_prob_smrtlink_sorted.cutoff.0.50.bed')\n",
    "hc_feature_fn_dict['tmb_5mC_0.50'] = os.path.join(DIRS['TEST_COV'], '5mC_hc_tombo_sorted.cutoff.0.50.bed')\n",
    "hc_feature_fn_dict['tmb_cpg_5mC_0.50'] = os.path.join(DIRS['TEST_COV'], '5mC_hc_tombo_sorted.CpG.cutoff.0.50.bed')\n",
    "hc_feature_fn_dict['np_5mC_0.50'] = os.path.join(DIRS['TEST_COV'], '5mC_hc_nanopolish_sorted.cutoff.0.50.bed')\n",
    "hc_feature_fn_dict['tmb_5mC_0.90'] = os.path.join(DIRS['TEST_COV'], '5mC_hc_tombo_sorted.cutoff.0.90.bed')\n",
    "hc_feature_fn_dict['tmb_cpg_5mC_0.90'] = os.path.join(DIRS['TEST_COV'], '5mC_hc_tombo_sorted.CpG.cutoff.0.90.bed')\n",
    "hc_feature_fn_dict['np_5mC_0.90'] = os.path.join(DIRS['TEST_COV'], '5mC_hc_nanopolish_sorted.cutoff.0.90.bed')                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_feature_bed_dict = {}\n",
    "for key, value in hc_feature_fn_dict.items():\n",
    "    hc_feature_bed_dict[key] = BedTool(value)\n",
    "\n",
    "pprint.pprint(hc_feature_bed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run coverage for higher stringency files\n",
    "hc_test_feature_overlap_df_dict = {}\n",
    "for wkey, wbed in test_window_bed_dict.items():\n",
    "    for fkey, fbed in hc_feature_bed_dict.items():\n",
    "        tmp_df = wbed.coverage(fbed).to_dataframe().iloc[:,[0,1,2,3,6]] # make a dataframe to put headings\n",
    "        tmp_df.rename(columns={'name': 'overlap_count', 'thickStart': 'overlap_fraction'}, inplace=True) # rename headings\n",
    "        tmp_fn = hc_feature_fn_dict[fkey].replace('.bed', '.%s.overlap.bed' % wkey) # change output file path\n",
    "        tmp_fn = tmp_fn.replace('test_feature_files', 'test_coverage')\n",
    "        hc_test_feature_overlap_df_dict[tmp_fn.split('/')[-1]] = tmp_df # file name as key and dataframe as value for overlap dict\n",
    "        tmp_df.to_csv(tmp_fn, sep='\\t', header=None, index=None) # save to a csv(pybedtools outputs more d.p. than BEDTools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running coverage for sliding windows\n",
    "# Make dictionary of test feature files to make the function easier\n",
    "# The necessary feature files were moved to their own folder first\n",
    "# Will need to move all feature files to one folder for actual analysis\n",
    "\n",
    "DIRS['TEST_COV'] = os.path.join(DIRS['BASE2'], 'coverage/test_feature_files')\n",
    "test_sliding_feature_fn_dict = {}\n",
    "test_sliding_feature_fn_dict['genes'] = os.path.join(DIRS['TEST_COV'], 'Pst_104E_v13_ph_ctg.anno.sorted.gff3')\n",
    "test_sliding_feature_fn_dict['TE'] = os.path.join(DIRS['TEST_COV'], 'Pst_104E_v13_ph_ctg.TE.sorted.gff3')\n",
    "test_sliding_feature_fn_dict['effector'] = os.path.join(DIRS['TEST_COV'], 'Pst_104E_v13_ph_ctg.effectors.gff3')\n",
    "test_sliding_feature_fn_dict['tmb_6mA_0.90'] = os.path.join(DIRS['TEST_COV'], '6mA_hc_tombo_sorted.cutoff.0.90.bed')\n",
    "test_sliding_feature_fn_dict['tmb_6mA_0.50'] = os.path.join(DIRS['TEST_COV'], '6mA_hc_tombo_sorted.cutoff.0.50.bed')\n",
    "test_sliding_feature_fn_dict['tmb_5mC_0.90'] = os.path.join(DIRS['TEST_COV'], '5mC_hc_tombo_sorted.cutoff.0.90.bed')\n",
    "test_sliding_feature_fn_dict['tmb_5mC_0.50'] = os.path.join(DIRS['TEST_COV'], '5mC_hc_tombo_sorted.cutoff.0.50.bed')\n",
    "test_sliding_feature_fn_dict['tmb_cpg_5mC_0.90'] = os.path.join(DIRS['TEST_COV'], '5mC_hc_tombo_sorted.CpG.cutoff.0.90.bed')\n",
    "test_sliding_feature_fn_dict['np_5mC_0.90'] = os.path.join(DIRS['TEST_COV'], '5mC_hc_nanopolish_sorted.cutoff.0.90.bed')\n",
    "test_sliding_feature_fn_dict['tmb_cpg_5mC_0.50'] = os.path.join(DIRS['TEST_COV'], '5mC_hc_tombo_sorted.CpG.cutoff.0.50.bed')\n",
    "test_sliding_feature_fn_dict['np_5mC_0.50'] = os.path.join(DIRS['TEST_COV'], '5mC_hc_nanopolish_sorted.cutoff.0.50.bed')\n",
    "test_sliding_feature_fn_dict['pb_6mA_0.90'] = os.path.join(DIRS['TEST_COV'], '6mA_prob_smrtlink_sorted.cutoff.0.90.bed')\n",
    "test_sliding_feature_fn_dict['pb_6mA_0.50'] = os.path.join(DIRS['TEST_COV'], '6mA_prob_smrtlink_sorted.cutoff.0.50.bed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert file paths to bed files\n",
    "test_sliding_feature_bed_dict = {}\n",
    "for key, value in test_sliding_feature_fn_dict.items():\n",
    "    test_sliding_feature_bed_dict[key] = BedTool(value)\n",
    "\n",
    "pprint.pprint(test_sliding_feature_bed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sliding_feature_overlap_df_dict = {}\n",
    "for wkey, wbed in test_sliding_window_bed_dict.items():\n",
    "    for fkey, fbed in test_sliding_feature_bed_dict.items():\n",
    "        tmp_df = wbed.coverage(fbed).to_dataframe().iloc[:,[0,1,2,3,6]] # make a dataframe to put headings\n",
    "        tmp_df.rename(columns={'name': 'overlap_count', 'thickStart': 'overlap_fraction'}, inplace=True) # rename headings\n",
    "        if tmp_fn.endswith('.bed'): # for methylation files\n",
    "            tmp_fn = test_sliding_feature_fn_dict[fkey].replace('.bed', '.%s.overlap.bed' % wkey) # change output file path\n",
    "        if tmp_fn.endswith('.gff3'): # for gene/transposon/effectors file\n",
    "            tmp_fn = test_sliding_feature_fn_dict[fkey].replace('.gff3', '.%s.overlap.bed' % wkey) # change output file path\n",
    "        tmp_fn = tmp_fn.replace('test_feature_files', 'test_sliding_coverage')\n",
    "        test_sliding_feature_overlap_df_dict[tmp_fn.split('/')[-1]] = tmp_df # file name as key and dataframe as value for overlap dict\n",
    "        tmp_df.to_csv(tmp_fn, sep='\\t', header=None, index=None) # save to a csv(pybedtools outputs more d.p. than BEDTools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run overlaps between windows and features\n",
    "feature_overlap_df_dict = {}\n",
    "for wkey, wbed in window_bed_dict.items():\n",
    "    for fkey, fbed in feature_bed_dict.items():\n",
    "        tmp_df = wbed.coverage(fbed, F=0.1).to_dataframe().iloc[:,[0,1,2,3,6]] #(F=0.1 indicates minimum fraction overlap)\n",
    "        tmp_df.rename(columns={'name': 'overlap_count', 'thickStart': 'overlap_fraction'}, inplace=True)\n",
    "        tmp_fn = feature_fn_dict[fkey].replace('bed', '%s.overlap.bed' % wkey)\n",
    "        feature_overlap_df_dict[tmp_fn.split('/')[-1]] = tmp_df\n",
    "        tmp_df.to_csv(tmp_fn, sep='\\t', header=None, index=None)\n",
    "        tmp_fn = feature_fn_dict[fkey].replace('bed', '%s.overlap.circabed' % wkey)\n",
    "        tmp_df.to_csv(tmp_fn, sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of file names to be dictionary keys and check if it worked (it did!)\n",
    "methylation_file_names = []\n",
    "for file in all_methylation_files:\n",
    "    name = file[58:]\n",
    "    methylation_file_names.append(name)\n",
    "methylation_file_names.sort()\n",
    "   \n",
    "pprint.pprint(methylation_file_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dictionary of bedtools objects and check if it worked (it did!)\n",
    "all_methylation_bed_dict = {}\n",
    "for i in range(len(all_methylation_files)):\n",
    "    all_methylation_bed_dict[methylation_file_names[i]] = BedTool(all_methylation_files[i])\n",
    "    \n",
    "pprint.pprint(all_methylation_bed_dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
